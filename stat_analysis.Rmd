---
title: "Text Data Mining for Detective Stories"
author: "Yun-Hsiang (Ray) Chan"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r, include = FALSE}
library(knitr)
library(tidyverse)
library(png)
library(philentropy)
library(LaplacesDemon)
library(coin)
```

# 1. Abstract

Detective story is a subgenre of crime and mystery fiction in which an investigator conducts investigation on a crime, always a murder. These detective stories occurred since mid-nineteenth century and has remained extremely popular over years.  

In this paper, we collaborate with researchers from Department of English Literature, collecting a dataset with over 400 manual annotations and text of different detective stories. Combining traditional statistical analysis as well as various text data mining methods, we focus on investigating the situation of affective states of detective stories with different components, the evolution of components in detective stories, and the potential text features that might differentiates the stories.  

In conclusion, although we can't specify the true factors that differentiates the stories, we successfully find the difference of topics over decades and prove that the affective states varies substantially when stories are composed of different components.

# 2. Introduction

Detective story is a type of crime and mystery fiction remains popular for decades. Over the decades, many researchers are strongly interested in analyzing the detective stories from different aspects. English literature researchers always want to know the most important component that contributes to a good detective stories, the evolution of detective stories over years, and the situation of affective states in detective stories with different components.  

Previously, researchers can only collect human-annotated dataset and conduct traditional statistical analysis such as hypothesis testing or regression analysis to do quantitative analysis. Recently, with the advancement of text data mining and natural language processing techniques, researchers start to utilize these novel techniques for finding hidden facts from the text data. In this paper, by combining traditional statistical analysis skills such as hypothesis testing and the latest text data mining techniques such as sentiment analysis, we try to tackle the evolution of detective stories and the situation of affective states in different detective stories. In the meanwhile, we also try to investigate the hidden text features that might differentiates the stories.  

For the remainder of the paper:  

- Section 3 is about the method, where 3.1 is about EDA method, 3.2 is about sentiment analysis method, 3.3 is about emotion analysis method, and 3.4 is about tf-idf feature extraction and clustering method.  

- Section 4 and its subsection are about the conclusion we obtain from each method section.  

- Section 5, the last section, is the conclusion and discussion of this research.  

For the analysis about evolution of detective stories, although we try to investigate the question through different methods (either in the sentiment or emotion analysis part, or in the clustering part), it's hard to see any trend since the size of the dataset is small, and our main conclusions are from the EDA of annotated dataset.  

For analysis about the situation of affective states, we reach our conclusion through sentiment analysis and emotion analysis.  

And lastly, for analysis about the hidden text features that might differentiates the stories, we conduct the investigation by doing clustering on tf-idf features, while there's no discrete conclusion that we can reach at the current stage. The question can be furthered investigated by other researchers in future.  



# 3. Methods

## 3.4 Clustering through tf-idf features

In this subsection, our goal is to assign clusters to detective stories based on their content. The main purpose is to dig out the potential text features that differentiates the stories, where our main expectation is that some stories are differentiating by their publication years.

There are 3 main steps for this method:  

1. Calculate tf-idf scores for each word (excluding some rare and common words) as the representation vector for each detective story.  
2. Implement K-means clustering algorithm for assigning the clusters.  
3. Plot out the clustering result in a low-dimensional space, as a preliminary of next step.  

These 3 steps will be dismantled into 6 subsections.  

We will introduce the idea, the implementation details and the conclusion from all steps above.  

## 3.4.1 Introduction to the idea of tf-idf

The tf-idf score, short for **term frequency-inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection of corpus. It's often used as a weighting factor in searches of information retrieval, text mining, and user modeling.  

The tfâ€“idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.  

The formula of tf-idf score of term $t$ in document $d$ is:
$$tf(t, d) = \frac{f_{t, d}}{\sum_{t'\in d}f_{t', d}}$$
$$f_{t,d} \text{ is the number of times that term t occurs in document occurs in document d}$$

**Intuition:**

- If a term occurs less frequently among all documents while occurring many times in this specific document, it indicates that this term is valuable to this document.

- For example, if the word "doctor" appears 100 times in this story and it only appears in 2 different stories, the tf-idf score is 50. The other word, "knife", appears 10 times in this story and it appears in 10 different stories, the tf-idf score is 10.  

- This indicates that "doctor" is a comparatively crucial word than "knife" to this story.

## 3.4.2 tf-idf scores as representation vector

After introducing the idea of tf-idf, it's time to introduce how can we use them as a representation vector.

### Popular method in information retrieval

Since tf-idf scores for each word can represent how important the word to this story, by forming them as a vector, it could be a representation learned from the text of the story.

### Implementation Details

To do that, for each document, we need to calculate the tf-idf score for every word in the document, and use these scores to form a vector.  

To eliminate the negative influence of some rare words or common words, we set the threshold for adding the word to the representation vector.  

- If a word appears in more than 50% of the documents or less than 80% of the documents, then the tf-idf score for this word in every document will be included in the representation vector.  

- Otherwise, the tf-idf score for this word won't be included.  

The reason to set threshold is to reduce the noise of common word like 'he' or 'she', and the noise of rare terms that might appear only in a specific type of stories (e.g. Holmes). The 50% and 80% are picked based on empirical experience.  

**We hope that by reducing these noises, the vector can capture the pattern of commonly occurred verb or noun so that we can detect the topic or writing habits of a particular era.** 

At the end, we have a vector with 425 dimensions for each detective story, with each dimension represent the tf-idf of the term. Each term must appear in more than 50% of documents, or less than 80% of documents.  

## 3.4.3 K-means clustering and Dimensionality reduction by MDS.

As we have the representation vectors, we can assign clusters for each story based on the vectors.  

The reason we do this step is to figure out why some stories are assigned to a specific cluster in the next part.  

Is it because of the publication year? Or it is due to authors, topics, or something we never notice before.  

### Implementation Details

The entire process was done in python. Comments are provided in Appendix 1.  

We implement K-means clustering to assign the cluster and draw the plots by:  

1. Prepare the tf-idf matrix, where each row represents a detective story, and each column represent the tf-idf score for a unique word.  

2. Calculate the distance between each row vectors by cosine similarity and form a matrix.  

3. Choose the number of clusters, and apply the K-means algorithm on both tf-idf matrix and distance matrix. 

4. Reduce the dimension of distance similarity to 2 by Multidimensional scaling (MDS). This is a technique to visualize the level of similarity between vectors.  

5. Draw the plots with publication year and author code as a preliminary insight.  

### Briefly Introduction to the methods 

- K-means clustering is a method of vector quantization, that aims to partition n observations into k clusters, wehre each observation belongs to the cluster with the nearest mean.  

- MDS is a means of visualizing the level of similarity of individual cases of a dataset. It's used to translate "information about the pairwise distances among a set of n objects" into a configuration of n points mapped into an abstract Cartesian space (2-dimensional space).  

### Why doing clustering on both tf-idf matrix and similarity distance matrix?

- The similarity distance matrix was originally computed for visualization. However, we find something interesting when doing the clustering.

- The clustering result will be mostly the same when applying k-means clustering on tf-idf and distance matrices. But those tiny differences raise our interest. 

- We believe both type of matrix can capture some special patterns that the other one can't capture. This is why we keep the result of clustering on both matrices.  

### Why clustering and why K-means? 

- The reason to do clustering is to differentiate the stories by text feature vectors so that we can find out the reason behind cluster. Clustering is one the most popular unsupervised methods for pattern recognition. This is the reason for choosing it.  

- In terms of clustering methods, we also try the hierarchical clustering method. The hierarchical clustering method will produce a similar clustering result for distance matrix, like the one we can obtain by K-means clustering, while it produces an inconsistent result for tf-idf matrix clustering. Therefore, to compare the clusters produced by two matrices at the same time, we believe K-means is more suitable in this case.  

## 4.4 Text Feature Clustering Results

### 4.4.1 Clustering EDA results

The two plots are clustering result on tf-idf matrix or distance matrix with author code.  

In these plots, each point represents one story, and each color represents the cluster.

Note:  

- Empirically, we select $num\_clusters = 7$ for both distance matrix and tf-idf matrix clustering.  

- The x and y axis in the plot have no actual meaning. They are only calculated to measure the distance between different stories.  

- The position of each story in the plot are the same because they are both calculated on the distance matrix, while the color will be different because of clustering on different matrices (tf-idf or distance).  

- The number of cluster will be slightly different by orders. This is because the clustering are done separately.  

We first look at the clustering plot by distance matrix and tf-idf matrix with author code information.  

```{r, echo = FALSE, out.width = "115%", fig.align = 'left'}
# Plot the graph. The graphs are from python.
img_path1 <- "data_package/cluster_plot_author_dist.png"
img_path2 <- 'data_package/cluster_plot_author_tfidf.png'
include_graphics(img_path1)
include_graphics(img_path2)
```

From these two plots, we can know that although we really hope that the clustering of stories can rely on different eras, **it's more obvious that stories from the same author are more likely to be assigned to one cluster.**

For example, the stories from JF (Jacques Furtelle) are likely to be assigned to one cluster (cluster 2 in plot 1 and cluster 3 in plot 2). It's reasonable that they are classified in one cluster when the writing habits of the author is unique. Moreover, the topics of these stories are similar (most of them are from the series of detective stories, thinking machine).

Other clusters based on authors are stories from ABR (Arthur B. Reeve) or MDP (Melville Davisson Post) by distance matrix clustering.

#### Interesting Facts

There are still some clusters are assigned with other reasons, and these reasons are not publication years.  

**Note: the plots with publication year information are provided in the Appendix 2. There are also comments for justifying my claim.**

#### Fact A: Cluster 1 and 5 in tf-idf matrix clustering

One interesting fact is that, while stories from ABR (Arthur B. Reeve) are assigned to the same cluster by distance matrix clustering, they are divided into different clusters by tf-idf matrix clustering. And from empirical experiments, if we choose *n_clusters = 6* (now it's 7), then these stories won't be divided into different clusters by tf-idf matrix clustering.  

This indicates that there are some crucial text features in tf-idf matrix that could be used to distinguish between cluster 1 and 5 in plot 2, while these features can't be captured by similarity distances.  

#### Fact B: Cluster 3 and 6 in distance matrix clustering

The other interesting fact is that, while cluster 2 from tf-idf matrix clustering (plot 2) looks like a combination of all remaining stories (this is even more obvious when *n_clusters = 3* or *4*), the similarity distance clustering  method (plot 1) can assign different clusters for them (cluster 3 and cluster 6).  

This indicates that there are some crucial features in similarity distance matrix that could be used to distinguish between cluster 3 and 6 (plot 1), while these features can't be captured by tf-idf features.  

## 2.6 Next Steps

Therefore, in the next few sections, we will investigate:  

1. What is differentiating cluster 3 and cluster 6 in plot 1 (similarity distance matrix clustering)?  

2. What is differentiating cluster 1 and cluster 5 in plot 2 (tf-idf matrix clustering)?  

### Implementation

We plan to conduct several hypothesis testings to see whether these clusters are assigned due to similar topics.

If there's no explicit reason from the annotated dataset, then the authenticate logic behind the classification should be left for the experts of detective stories, our collaborators.

### Other choices and limitations.

- It's also viable to look at the most influential column values (i.e. the tf-idf scores for some words) contributes to each cluster after tf-idf matrix clustering, and try to find out the reason from the word.   

- However, these influential tf-idf scores are often noisy. There might exist some patterns when scanning them altogether, but these patterns are hard to be observed individually.  

- Therefore, as we already have annotation of each story, we instead take the annotations for investigation.  

- You can also find the most influential words (top 10) to each cluster for tf-idf matrix clustering in Appendix 3.  

- Similarity distance clustering do not have these words, since the dimension is changed when calculating the matrix from tf-idf matrix and therefore the dimensions don't have actual meanings.  

# III. Statistical Anlaysis

In this section, we will mainly focus on hypothesis testing. We want to testify whether the implicit logic behind the grouping. For two the groups of clusters, are they assigned because they have similar topics or written in different eras?

## 3.1 Fisher's Exact Test of Independence

For examining the topics of two groups (cluster 3 / 6 from similarity distance matrix clustering, and cluster 1 / 5 from tf-idf matrix clustering), we can will specifically focus on the crime type and motive in the stories. These two components are the most representative annotations in our dataset.  

Let's first look at the two tables below.  

```{r, echo = FALSE}
# Read the data
crime_data <- read.csv("data_package/data/crime_types.csv")
motive_data <- read.csv("data_package/data/motive_types.csv")

dist_cluster_data <- read.csv('data_package/cluster_df_dist.csv')
dist_cluster_data <- dist_cluster_data %>% select(story_code, label)
tfidf_cluster_data <- read.csv('data_package/cluster_df_tfidf.csv')
tfidf_cluster_data <- tfidf_cluster_data %>% select(story_code, label)

# Filter the clusters we want from different clustering method.
cluster_crime <- left_join(dist_cluster_data, crime_data, by = 'story_code') %>% 
  filter(label == 3 | label == 6)

cluster_crime_2 <- left_join(tfidf_cluster_data, crime_data, by = 'story_code') %>% 
  filter(label == 1 | label == 5)

# Count the number of each crime type for these two groups
grouped_crime <- cluster_crime %>% 
  group_by(label) %>% 
  summarise(count = n(), across(where(is.numeric), ~ sum(.x, na.rm = FALSE))) %>% 
  select(!publication_year) %>% 
  select(!score) %>% 
  rename(n_stroies = count)

grouped_crime_2 <- cluster_crime_2 %>% 
  group_by(label) %>% 
  summarise(count = n(), across(where(is.numeric), ~ sum(.x, na.rm = FALSE))) %>% 
  select(!publication_year) %>% 
  select(!score) %>% 
  rename(n_stroies = count)

# Create new df for visualization
table_df_crime <- data.frame(`Crime Types` = names(grouped_crime[, 3:16]), `Cluster 3` = as.numeric(grouped_crime[1, 3:16]), `Cluster 6` = as.numeric(grouped_crime[2, 3:16]), `Cluster 1` = as.numeric(grouped_crime_2[1, 3:16]), `Cluster 5` = as.numeric(grouped_crime_2[2, 3:16]))

# Filter the clusters we want from different clustering method.
cluster_motive <- left_join(dist_cluster_data, motive_data, by = 'story_code') %>% 
  filter(label == 3 | label == 6)

cluster_motive_2 <- left_join(tfidf_cluster_data, motive_data, by = 'story_code') %>% 
  filter(label == 1 | label == 5)

# Count the number of each motive type for these two groups
grouped_motive <- cluster_motive %>% 
  group_by(label) %>% 
  summarise(count = n(), across(where(is.numeric), ~ sum(.x, na.rm = FALSE))) %>% 
  select(!publication_year) %>% 
  select(!score) %>% 
  rename(n_stroies = count)

grouped_motive_2 <- cluster_motive_2 %>% 
  group_by(label) %>% 
  summarise(count = n(), across(where(is.numeric), ~ sum(.x, na.rm = FALSE))) %>% 
  select(!publication_year) %>% 
  select(!score) %>% 
  rename(n_stroies = count)

# Create new df for visualization
table_df_motive <- data.frame(`Motive Types` = names(grouped_motive[, 3:14]), `Cluster 3` = as.numeric(grouped_motive[1, 3:14]), `Cluster 6` = as.numeric(grouped_motive[2, 3:14]), `Cluster 1` = as.numeric(grouped_motive_2[1, 3:14]), `Cluster 5` = as.numeric(grouped_motive_2[2, 3:14]))
```

```{r, echo = FALSE}
knitr::kable(table_df_crime)
knitr::kable(table_df_motive)
```

```{r}
cluster_crime <- left_join(dist_cluster_data, crime_data, by = 'story_code') %>% 
  filter(label == 3 | label == 6)
cluster_crime %>% arrange(publication_year) %>% select(story_name, label, publication_year, author_code)
```


From these two tables, we can know that for either group, the two clusters are not coming from a particular crime type or motive type.  

Therefore, we would like to use Fisher exact test of independence to check whether the distributions of crime and motive types from these two clusters in each group are independent.  

The p-value for the test of each group are below.  

```{r, include = FALSE}
# Note, since the fisher test required random simulation, the p-value won't be exactly equivalent, but it will reach the same conclusion.
fisher.test(table_df_crime %>% select(`Cluster.3`, `Cluster.6`), simulate.p.value = TRUE)
fisher.test(table_df_crime %>% select(`Cluster.1`, `Cluster.5`), simulate.p.value = TRUE)
fisher.test(table_df_motive %>% select(`Cluster.3`, `Cluster.6`), simulate.p.value = TRUE)
fisher.test(table_df_motive %>% select(`Cluster.1`, `Cluster.5`), simulate.p.value = TRUE)
```

```{r, echo = FALSE}
g <- c("Cluster 3 and 6 from dist clustering", "Cluster 1 and 5 from tf-idf clustering")
p1 <- c(0.001, 0.87)
p2 <- c(0.02, 0.50)
kable(data.frame(`Group` = g, `p-value for crime distribution test` = p1, `p-value for motive distribution test` = p2))
```

From the p-value above, we can conclude that:  

1. Overall, stores in cluster 3 and 6 from distance clustering are more likely to have different topics.  

  - The p-values for the test on crime and motive type distributions of cluster 3 and 6 are smaller than 0.05.  

  - It's a strong evidence against the null hypothesis that these two groups are from the same distribution (topic).  

2. Overall, stories in cluster 1 and 5 from tf-idf matrix clustering are more likely to have similar topics.  

  - The p-values for the test on crime and motive type distributions of cluster 1 and 5 are larger than 0.05.  

  - We fail to reject the null hypothesis that these two groups are from the same distribution (topic).  

### Choice of testing methods

#### Why choosing fisher's exact test 

- We want to see whether they two clusters are having similar topic.  

- It's hard to define topic by the annotations we have. However, we can look at the overall distribution of crime and topic as an overview of topics in the cluster.  

#### Why not choosing chi-square test?

- The reason for not doing chi-square test is simple. Chi-square test is more useful when the sample size is large, while here these clusters are specifically assigned from our the small-size sample.

- Therefore, fisher's exact test is more suitable under this case.  

### In-depth Interpretation of test result

- If the clusters are from similar distribution, it indicates that there might be some commonly appeared word (e.g. police) in these particular topics.  

- If the clusters are from different distribution, it indicates that there might exist some other reasons (other linguistic features) that push the clustering algorithm to divide them.  

- Either result could become the next research question for our collaborators.  

## 3.2 Next Steps for the collaborators

The analysis is ended in this part, since further investigation is too difficult without the domain knowledege of detective stories.  

Here are some questions that could be investigated further by our collabortors:

**1. The two clusters have different topics, then what are the implicit logic behind the assignment of cluster 3 and 6?**   

What elements (either components of stories or linguistic features) lead to the special representation vectors that could be classified into different groups?  

**2. What are the explicit language features in those specific topics from cluster 1 and 5?**  

Topic is a broad definition here. Can we define them in a more specific way? What are the key factors lead to the final assignment?  

# IV. Explanation of Doing an Unconventional Analysis & Limitations

In my opinion, the main component of this analysis relies on the idea of text analysis and the choices of method to conduct the analysis (tf-idf, clustering, dimensionality reduction for visualization).  

These methods (e.g. clustering) include some statistical component, while it's not the traditional testing or modeling expected in a general statistical analysis. This is mainly because of the characteristic of text data that I want to analyze.  

I choose this topic as a challenge of learning new things and conduct insightful analysis from the text of detective stories, rather than the annotated dataset.  

The design and choice of hypothesis test in the analysis part is not completely reasonable. The test itself and some assumptions might not be suitable for the research question. This is mainly because of the unconventional dataset we have and the difficult question that we want to investigate.  

However, it's too hard to find the pattern and analyze further without the insight and domain knowledge from our collaborators. The only choice to keep the analysis part is to include these tests.  

Therefore, even if the tests are not absolutely reasonable, I still keep them in the report and structure the report as the one that is originally expected in STA490.  

I still believe that to obtain a more meaningful text data analysis, it's better to communicate and cooperate with our collaborators for in-depth understanding of the stories (those clusters) rather than doing the testing like this analysis.  

I hope my decisions wouldn't be criticized and hurt the grade of this project.  



# V. Sentiment Analysis 

In this section, we mainly focus on another research question: whether the sentiments in the first and second portion of detective stories change over years.

## 5.1 Data Cleaning

We first clean the data by unicode normalization and manual check of punctuation, and eventually find the reveal border sentences recorded in the input_form.csv. The issues encountered are provided below:

1. There are some annotations that don't match the stories (i.e. the reveal border sentence can't be found successfully). These stories are: 'OMIC04', 'ASH09', 'OMIC03', 'CKS53', 'PVDS41', 'CBSH05'. The problem for each of them are provided in project log.  


2. There are some stories without an explicit reveal border sentence provided in the input_form.csv. These stories are:  'ASH01', 'CBSH10', 'LMSY03', 'MC03', 'GPM01', 'OSH05', 'TSOTR15', 'TEV02_02', 'TEV02_01'.  

3. There are some stories provided but not annotated in the input_form.csv. These stories are These stories are: 'TEV02': 'TEV02 - Cassie CÃ´tÃ©(1).txt', 'TCD03': 'TCD03 - Wen W..txt', 'TCD02': 'TCD02 - Wen W..txt', 'TCD01': 'TCD01 - Wen W..txt'.  

4. There are some stories annotated but aren't provided as the plain texts. These stories are: 'CKS21'.

Apart from these problematic annotations or stories, we get the first and second (investigation & reveal) portions of each detective story annotated in the input_form.csv.

## 5.2 Lexicon-based Sentiment Analysis

For the sentiment analysis, we utilize the 'nltk' and 'vaderSentiment' package in python. The steps are:  

1. Perform sentence tokenization (i.e. separate a complete paragraph to different sentences) by nltk.  

2. Clean each sentence by removing punctuation and number.  

3. Perform word tokenization (i.e. separate a complete sentence to different words) on each sentence by nltk.  

4. Do pos tagging (i.e. categorizing words in a text in correspondence with a particular part of speech) by nltk.  

5. Remove stop words (i.e. the most common words in any language) by nltk.  

6. Do lemmatization (i.e. grouping together the inflected forms of a word so they can be analysed as a single item. For example, make 'am', 'is', 'are' -> 'be') by ntlk.  

7. Perform sentiment analysis on the lemma list of each sentence by vaderSentiment. The package provides the analysis for each lemma list and give a score.  

- The score is restricted between -1 and 1.  

- A score greater than or equal to 0.5 implies a positive sentence.  

- A score less than or equal to -0.5 implies a negative sentence.  

- Otherwise, it implies a neutral sentence.  

8. Record the proportion of positive, negative and neutral sentences of each detective stories in each portion. Save the csv file.  

## 5.3 Trend Analysis

```{r, include = FALSE}
senti <- read.csv("data_package/sentiment.csv")
senti <- left_join(senti, crime_data %>% rename(code = 'story_code') %>% select(code, publication_year), on = 'code')
```

Now it's time to look at the trend of these proportions. These two plots are the line plot of proportion of positive, negative, and neutral sentences in the first and second portion changes over year. From these two plots, it's obvious that there is no explicit trend.

```{r, echo = FALSE, out.width = "50%"}
grouped_senti <- 
  aggregate(senti[, 2:9], list(senti$publication_year), mean) %>% 
  mutate_if(is.double, round, digits = 2) %>% 
  rename(publication_year = 'Group.1')

grouped_senti %>% ggplot(aes(x = publication_year)) + geom_line(aes(y = first_negative, color = 'Negative')) + geom_line(aes(y = first_positive , color = "Positive")) + geom_line(aes(y = first_neutral, color = "Neutral")) + xlab("Publication Year") + ylab("Proportion of sentences") + ggtitle("The Proportion of Positive, Negative, Neutral sentences in first portion by Year")

grouped_senti %>% ggplot(aes(x = publication_year)) + geom_line(aes(y = second_negative, color = 'Negative')) + geom_line(aes(y = second_positive , color = "Positive")) + geom_line(aes(y = second_neutral, color = "Neutral")) + xlab("Publication Year") + ylab("Proportion of sentences") + ggtitle("The Proportion of Positive, Negative, Neutral sentences in second portion by Year")
```
To compare the difference of each sentiment between first and second proportion, we group the publication year by decade as follow:  

```{r, echo = FALSE}
decade_senti <- senti %>% 
  mutate(decade = ifelse(publication_year <= 1900, "1891-1900", ifelse(publication_year <= 1910, "1901-1910", "1911-1927")),
         decade_level = ifelse(publication_year <= 1900, 1, ifelse(publication_year <= 1910, 2, 3)))

t <- decade_senti %>% 
  group_by(decade, decade_level) %>% 
  summarise(count = n(), .groups = "drop") %>% 
  filter(!is.na(decade)) %>% select(decade, decade_level, count)

knitr::kable(t)
```

And the plots of mean proportion of different sentiments in different decade are plotted below.  

```{r, echo = FALSE, out.width = "50%"}
aggregate_senti <- aggregate(decade_senti[, 2:9], list(decade_senti$decade_level), mean) %>% rename(decade_level = "Group.1")

# grouped_senti %>% ggplot(aes(x = publication_year)) + geom_line(aes(y = first_negative, color = 'Negative in first portion')) + geom_line(aes(y = second_negative , color = "Negative in second portion")) + xlab("Publication Year") + ylab("Proportion of sentences") + ggtitle("The Proportion of Negative sentences in the first and second portion by Year")

# grouped_senti %>% ggplot(aes(x = publication_year)) + geom_line(aes(y = first_positive, color = 'Positive in first portion')) + geom_line(aes(y = second_positive , color = "Positive in second portion")) + xlab("Publication Year") + ylab("Proportion of sentences") + ggtitle("The Proportion of Positive sentences in the first and second portion by Year")

aggregate_senti %>% ggplot(aes(x = decade_level)) + geom_line(aes(y = first_negative, color = 'Negative in first portion')) + geom_line(aes(y = second_negative , color = "Negative in second portion")) + xlab("Publication Year") + ylab("Mean Proportion of sentences") + ggtitle("The Mean Proportion of Negative sentences in the first and second portion by Decade")

aggregate_senti %>% ggplot(aes(x = decade_level)) + geom_line(aes(y = first_positive, color = 'Positive in first portion')) + geom_line(aes(y = second_positive , color = "Positive in second portion")) + xlab("Publication Year") + ylab("Mean Proportion of sentences") + ggtitle("The Mean Proportion of Positive sentences in the first and second portion by Decade")

aggregate_senti %>% ggplot(aes(x = decade_level)) + geom_line(aes(y = first_neutral, color = 'Neutral in first portion')) + geom_line(aes(y = second_neutral , color = "Neutral in second portion")) + xlab("Publication Year") + ylab("Mean Proportion of sentences") + ggtitle("The Mean Proportion of Nuetral sentences in the first and second portion by Decade")
```

From these three plots, we can know that the trend is still not obvious between each decade. Therefore, we can conclude that the sentiments in different portions of decective stories didn't change a lot over years.  

# VI. XGBoost for Choosing Readers' Favorite Topics

In this section, we utilize XGBoost Classifier for choosing the readers' (annotators') favorite topics (components of stories). 

## 6.1 Introduction to XGBoost

XGBoost (Extreme Gradient Boosting) is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.  

The reason to choose this method is that decision tree based algorithms are considered the best type of algorithms for handling small-to-medium tabular data, and XGBoost is considered the best one among them in the Kaggle competition. It outperforms most on the model and trains very fast.

## 6.2. Construct XGBoost Classifiers for Readers' Recommendation

The labels for training XGBoost is "Would you recommend this story to a friend", which is a binary classification problem.  

To examine the influence of each components of the stories, we train 3 different XGBoost Classifiers for motive types, crime types and other potential categorical variables in the input_form.csv dataset.  

We didn't construct one classifier for all these variables, because the connections between these three factors is not obvious and we want to focus on the internal influences from each of these three factors itself.

## 6.3

\newpage

## Appendix 1: Data Preparing Process

To deal with the text data, we utilize python instead of R. I upload all python code, R code, produced csv and plots in github repository (https://github.com/rachan1637/detective).  

The preprocessing includes the text data normalization and partitions. The partition-by-reveal-border process was initially designed for doing sentiment analysis, which was mentioned by the collaborators. But since the target for this task is vague, and unsupervised sentiment analysis is harder than expected, we give up the plan eventually. 

You can do 'python \{path\}/data_pacakage/main.py' in the terminal / bash to run the process of doing preprocessing, tf-idf counting, clustering and plotting. The path inside the python program might be changed, depending on the directory root that you run the program.  

I was too busy in these few weeks. As the grading rubric doesn't ask for a cleaned python code, I didn't add much annotations and clean my python code for your convenience. However, I am willing to do that if the TA, the instructor or the collaborators want to trace my code in future.  

### Appendix 2: Clustering Plots with Publication Year information

```{r, echo = FALSE,  out.width = "115%", fig.align = 'center'}
img_path1 <- "data_package/cluster_plot_year_dist.png"
img_path2 <- 'data_package/cluster_plot_year_tfidf.png'
include_graphics(img_path1)
include_graphics(img_path2)
```

Why we exclude pubilcation year as a reason of clustering:  

- We know that the collection of the dataset is biased, only the detective stories that could be found on the Internet are collected.  

- The reason that the clusters seem to be assigned due to the year is that many stories written by the same author were published in adjacent year, and all stories we have in some particular years are from only one or two authors. This conclusion can be proved by the plot with author information above.  

- Therefore. The publication year should not be considered as the primary reason of clustering.  

## Appendix 3: Top words contributes to tf-idf matrix clustering 

```{r, include = FALSE}
# Cluster 0 words:  
c0 <- c("replied", "cried", "big", "dead", "father", "girl", "woman", "sir", "fingers", "chair")

#Cluster 1 words:
c1 <- c("miss", "father", "woman", "girl", "mrs", "lady", "window", "herself", "replied", "paper")

# Cluster 2 words:
c2 <- c("sir", "friend", "police", "london", "window", "round", "paper", "street", "father", "business")

# Cluster 3 words
c3 <- c("thinking", "miss", "detective", "didn", "woman", "mrs", "replied", "girl", "clock", "money")

# Cluster 4 words:
c4 <- c("mrs", "woman", "lady", "wife", "miss", "girl", "police", "herself", "replied", "money")

# Cluster 5 words:
c5 <- c("doctor", "new", "mrs", "quickly", "miss", "replied", "remarked", "air", "body", "blood")

# Cluster 6 words:
c6 <- c("lady", "miss", "woman", "girl", "dear", "london", "police", "sir", "wife", "herself")
```

```{r, echo = FALSE}
# Create the table.
knitr::kable(data.frame(c0, c1, c2, c3, c4, c5, c6), col.names = c("Cluster 0", "Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5", "Cluster 6"), caption = "Top-10 influential words for each cluster")
```
